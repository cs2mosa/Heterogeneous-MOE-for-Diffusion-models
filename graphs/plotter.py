"""
Professional Analysis & Visualization Toolkit
=============================================
Compatible with Enhanced Logger (JSONL format)
"""

import torch
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import os
import json
from tqdm import tqdm
from matplotlib.gridspec import GridSpec
from scipy.ndimage import gaussian_filter1d
from typing import Optional
import warnings

warnings.filterwarnings('ignore')


class Plotter:
    """
    Research-grade visualization toolkit for hybrid diffusion MoE models.
    Reads JSONL logs generated by the Logger class.
    """

    def __init__(self, run_name: str = "experiment", output_dir: str = "./analysis_results"):
        self.run_name = run_name
        self.output_dir = os.path.join(output_dir, run_name)
        os.makedirs(self.output_dir, exist_ok=True)

        # Set publication-quality plotting style
        try:
            plt.style.use('seaborn-v0_8-paper')
        except OSError:
            plt.style.use('seaborn-paper')  # Fallback for older matplotlib

        sns.set_palette("husl")

        # Publication-quality settings
        plt.rcParams.update({
            'font.size': 11,
            'axes.labelsize': 12,
            'axes.titlesize': 13,
            'xtick.labelsize': 10,
            'ytick.labelsize': 10,
            'legend.fontsize': 10,
            'figure.titlesize': 14,
            'figure.dpi': 300,
            'savefig.dpi': 300,
            'savefig.bbox': 'tight',
            'axes.grid': True,
            'grid.alpha': 0.3,
            'lines.linewidth': 2,
        })

        print(f"✓ Initialized Professional Plotter for: {run_name}")
        print(f"✓ Output directory: {self.output_dir}")

    # =========================================================
    # 1. COMPREHENSIVE TRAINING DYNAMICS
    # =========================================================
    def plot_comprehensive_training_dynamics(self, log_file_path: str, smooth_window: int = 50):
        """
        Creates a comprehensive 9-panel training dynamics visualization.
        Reads from the main training log file.
        """
        df = self._parse_jsonl(log_file_path)

        if df is None or len(df) == 0:
            print("❌ No valid training data found")
            return

        # Map Logger keys to Plotter standard names
        # Logger keys: 'loss', 'denoising', 'balance', 'z_loss', 'entropy', 'log_var', 'zeta', 'lr'
        column_map = {
            'step': 'Step',
            'loss': 'Loss',
            'denoising': 'MSE',  # Raw denoising loss acts as MSE
            'pure_loss': 'EDMLoss',
            'balance': 'BalanceLoss',
            'z_loss': 'ZLoss',
            'entropy': 'EntropyLoss',
            'log_var': 'LogVar',
            'zeta': 'Zeta',
            'lr': 'LR'
        }
        df.rename(columns=column_map, inplace=True)

        # Smooth curves
        df_smooth = df.copy()
        smooth_cols = ['Loss', 'MSE', 'LogVar', 'EDMLoss', 'BalanceLoss', 'ZLoss', 'EntropyLoss']

        for col in smooth_cols:
            if col in df.columns and df[col].notna().any():
                # Handle sigma for gaussian filter based on data length
                sigma = max(2, min(smooth_window // 10, len(df) // 20))
                df_smooth[col] = gaussian_filter1d(df[col].values, sigma=sigma)

        fig = plt.figure(figsize=(20, 14))
        gs = GridSpec(4, 3, figure=fig, hspace=0.35, wspace=0.3)

        # --- Panel 1: Total Loss vs Pure EDM Loss ---
        ax1 = fig.add_subplot(gs[0, :2])
        if 'Loss' in df.columns:
            ax1.plot(df['Step'], df['Loss'], alpha=0.3, color='tab:blue', linewidth=0.5)
            ax1.plot(df_smooth['Step'], df_smooth['Loss'], color='tab:blue', linewidth=2, label='Total Loss')

        if 'EDMLoss' in df.columns:
            ax1.plot(df_smooth['Step'], df_smooth['EDMLoss'], color='tab:green', linewidth=2, label='Pure EDM Loss',
                     linestyle='--')

        ax1.set_xlabel('Training Step')
        ax1.set_ylabel('Loss Value')
        ax1.set_title('Training Loss Evolution')
        ax1.legend(loc='upper right')

        # --- Panel 2: Convergence Rate ---
        ax2 = fig.add_subplot(gs[0, 2])
        if 'Loss' in df_smooth.columns:
            loss_derivative = np.gradient(df_smooth['Loss'])
            ax2.plot(df['Step'], loss_derivative, color='tab:orange', linewidth=1.5)
            ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)
            ax2.set_title('Loss Convergence Rate')
            ax2.set_ylabel('Gradient (∂L/∂step)')

        # --- Panel 3: MSE (Log Scale) ---
        ax3 = fig.add_subplot(gs[1, 0])
        if 'MSE' in df.columns:
            ax3.semilogy(df['Step'], df['MSE'], alpha=0.3, color='tab:green', linewidth=0.5)
            ax3.semilogy(df_smooth['Step'], df_smooth['MSE'], color='tab:green', linewidth=2, label='MSE')
            ax3.set_title('Reconstruction Quality (MSE)')

        # --- Panel 4: Uncertainty (LogVar) ---
        ax4 = fig.add_subplot(gs[1, 1])
        if 'LogVar' in df.columns:
            ax4.plot(df['Step'], df['LogVar'], alpha=0.3, color='tab:red', linewidth=0.5)
            ax4.plot(df_smooth['Step'], df_smooth['LogVar'], color='tab:red', linewidth=2, label='Log Variance')
            ax4.set_title('Model Confidence (LogVar)')

        # --- Panel 5: Zeta Schedule ---
        ax5 = fig.add_subplot(gs[1, 2])
        if 'Zeta' in df.columns:
            ax5.plot(df['Step'], df['Zeta'], color='tab:purple', linewidth=2)
            ax5.fill_between(df['Step'], 0, df['Zeta'], alpha=0.3, color='tab:purple')
            ax5.set_title('Router Exploration (Zeta)')

        # --- Panel 6: MoE Auxiliary Losses ---
        ax6 = fig.add_subplot(gs[2, 0])
        aux_keys = ['BalanceLoss', 'ZLoss', 'EntropyLoss']
        has_aux = False
        for key in aux_keys:
            if key in df_smooth.columns:
                ax6.plot(df['Step'], df_smooth[key], label=key, linewidth=2)
                has_aux = True

        if has_aux:
            ax6.set_yscale('log')
            ax6.legend()
            ax6.set_title('MoE Auxiliary Losses')
        else:
            ax6.text(0.5, 0.5, 'No Aux Data', ha='center', transform=ax6.transAxes)

        # --- Panel 7: Learning Rate ---
        ax7 = fig.add_subplot(gs[2, 1])
        if 'LR' in df.columns:
            ax7.plot(df['Step'], df['LR'], color='tab:brown', linewidth=2)
            ax7.set_title('Learning Rate')
            ax7.set_yscale('log')

        # --- Panel 8: Pie Chart (Last Step) ---
        ax8 = fig.add_subplot(gs[2, 2])
        if 'Loss' in df.columns:
            final = df.iloc[-1]
            # Try to reconstruct components
            edm = final.get('EDMLoss', 0)
            balance = final.get('BalanceLoss', 0)
            zloss = final.get('ZLoss', 0)
            entropy = final.get('EntropyLoss', 0)

            # If EDM is 0 (not logged), assume remaining is EDM
            aux_sum = balance + zloss + entropy
            if edm == 0: edm = final['Loss'] - aux_sum

            if final['Loss'] > 0:
                sizes = [edm, aux_sum]
                labels = [f'Main\n{edm / final["Loss"] * 100:.1f}%', f'Aux\n{aux_sum / final["Loss"] * 100:.1f}%']
                ax8.pie(sizes, labels=labels, autopct='', startangle=90, colors=['#4CAF50', '#FF9800'])
                ax8.set_title('Final Loss Composition')

        # --- Panel 9: Stats Table ---
        ax9 = fig.add_subplot(gs[3, :])
        ax9.axis('off')

        stats = [['Metric', 'Initial', 'Final', 'Min', 'Max']]
        for col in ['Loss', 'MSE', 'EDMLoss', 'LogVar']:
            if col in df.columns:
                row = [col,
                       f"{df[col].iloc[0]:.4f}",
                       f"{df[col].iloc[-1]:.4f}",
                       f"{df[col].min():.4f}",
                       f"{df[col].max():.4f}"]
                stats.append(row)

        table = ax9.table(cellText=stats, cellLoc='center', loc='center', bbox=[0, 0, 1, 1])
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 1.5)

        # Style table headers
        for i in range(len(stats[0])):
            table[(0, i)].set_facecolor('#4472C4')
            table[(0, i)].set_text_props(weight='bold', color='white')

        fig.suptitle(f'Comprehensive Training Dynamics: {self.run_name}', fontsize=16, fontweight='bold', y=0.995)
        plt.savefig(f"{self.output_dir}/01_training_dynamics.png")
        print(f"✓ Saved: 01_training_dynamics.png")
        plt.close()

    # =========================================================
    # 2. EXPERT SPECIALIZATION HEATMAP
    # =========================================================
    def plot_expert_specialization_advanced(self, model, device='cuda', num_sigma_points=100):
        """
        Runs inference to visualize expert specialization across noise levels.
        """
        print("Analyzing Expert Specialization...")
        try:
            model.eval()

            # Generate sigma range (log-spaced)
            sigmas = torch.logspace(np.log10(0.002), np.log10(80), num_sigma_points).to(device)

            # Dummy inputs
            # Access internal channels from model config if available, else guess
            channels = getattr(model, 'internal_channels', 128)
            dummy_x = torch.randn(1, channels, 32, 32).to(device)
            dummy_time = torch.zeros(1, 256).to(device)  # assumes time_emb_dim=256

            unet_activations = []
            vit_activations = []

            # Handle unwrapped model vs DDP wrapper
            net = model.net if hasattr(model, 'net') else model
            net = net.module if hasattr(net, 'module') else net

            num_experts = len(net.Unet_experts)

            with torch.no_grad():
                for sigma in tqdm(sigmas, desc="Scanning noise levels"):
                    mask = torch.ones(1, num_experts).to(device)

                    # Run Routers Only
                    # Note: We pass zeta=0.0 for deterministic visualization
                    _, unet_probs, _ = net.Unet_router(x=dummy_x, time_emb=dummy_time, zeta=0.0, mask=mask)
                    _, vit_probs, _ = net.vit_router(x=dummy_x, time_emb=dummy_time, zeta=0.0, mask=mask)

                    unet_activations.append(unet_probs.cpu().numpy()[0])
                    vit_activations.append(vit_probs.cpu().numpy()[0])

            # Plotting Logic
            U_act = np.stack(unet_activations).T  # (Experts, Steps)
            V_act = np.stack(vit_activations).T

            fig = plt.figure(figsize=(20, 10))
            gs = GridSpec(2, 2, figure=fig)

            # UNet Heatmap
            ax1 = fig.add_subplot(gs[0, 0])
            im1 = ax1.imshow(U_act, aspect='auto', cmap='plasma', vmin=0, vmax=1)
            ax1.set_title('UNet Expert Activation vs Noise')
            ax1.set_ylabel('Expert ID')
            ax1.set_xlabel('Noise Level (Low -> High)')
            plt.colorbar(im1, ax=ax1)

            # ViT Heatmap
            ax2 = fig.add_subplot(gs[0, 1])
            im2 = ax2.imshow(V_act, aspect='auto', cmap='viridis', vmin=0, vmax=1)
            ax2.set_title('ViT Expert Activation vs Noise')
            plt.colorbar(im2, ax=ax2)

            # Utilization Bars
            ax3 = fig.add_subplot(gs[1, :])
            width = 0.35
            x = np.arange(num_experts)
            ax3.bar(x - width / 2, U_act.mean(axis=1), width, label='UNet Avg Usage', color='tab:orange')
            ax3.bar(x + width / 2, V_act.mean(axis=1), width, label='ViT Avg Usage', color='tab:blue')
            ax3.axhline(1.0 / num_experts, color='red', linestyle='--', label='Ideal Balance')
            ax3.set_title('Average Expert Utilization')
            ax3.set_xticks(x)
            ax3.legend()

            plt.suptitle(f"Expert Specialization: {self.run_name}")
            plt.savefig(f"{self.output_dir}/02_expert_specialization.png")
            print(f"✓ Saved: 02_expert_specialization.png")

        except Exception as e:
            print(f"❌ Failed to run expert analysis: {e}")
            print("  (Check model input shapes or device compatibility)")

    # =========================================================
    # 3. SCALING FACTORS & GATING ANALYSIS
    # =========================================================
    def plot_scaling_and_gating_analysis(self, log_file_path: str):
        """
        Analyzes scaling/gating columns from the main training log.
        """
        print("Analyzing Scaling & Gating...")
        df = self._parse_jsonl(log_file_path)
        if df is None: return

        # Check if required columns exist
        req_cols = ['scaling_vit_mean', 'scaling_unet_mean', 'gate_wx', 'gate_wa', 'step']
        if not all(col in df.columns for col in req_cols):
            print("⚠️ Scaling/Gating columns not found in log file.")
            return

        fig = plt.figure(figsize=(16, 12))
        gs = GridSpec(3, 2, figure=fig, hspace=0.3)

        # --- 1. Scaling Factors Evolution ---
        ax1 = fig.add_subplot(gs[0, :])
        ax1.plot(df['step'], df['scaling_vit_mean'], label='ViT Scale', color='tab:blue', alpha=0.7)
        ax1.plot(df['step'], df['scaling_unet_mean'], label='UNet Scale', color='tab:orange', alpha=0.7)
        ax1.set_title('Scaling Factors Evolution (Avg per batch)')
        ax1.set_xlabel('Step')
        ax1.legend()

        # --- 2. Scaling vs Noise Level (if available) ---
        if 'noise_level' in df.columns:
            ax2 = fig.add_subplot(gs[1, 0])
            # Scatter plot coloring by step to see time evolution
            sc = ax2.scatter(df["avg_sigma_percentile"], df['scaling_vit_mean'], c=df['step'], cmap='viridis', s=10, alpha=0.5)
            ax2.set_title('ViT Scaling vs Noise Level')
            ax2.set_xlabel('Noise Percentile (Avg)')
            ax2.set_ylabel('ViT Scale')
            plt.colorbar(sc, ax=ax2, label='Step')

            ax3 = fig.add_subplot(gs[1, 1])
            sc2 = ax3.scatter(df["avg_sigma_percentile"], df['scaling_unet_mean'], c=df['step'], cmap='plasma', s=10, alpha=0.5)
            ax3.set_title('UNet Scaling vs Noise Level')
            ax3.set_xlabel('Noise Percentile (Avg)')
            plt.colorbar(sc2, ax=ax3, label='Step')

        # --- 3. Gating Weights ---
        ax4 = fig.add_subplot(gs[2, 0])
        ax4.plot(df['step'], df['gate_wx'], label='Wx (UNet)', color='green')
        ax4.plot(df['step'], df['gate_wa'], label='Wa (Attn)', color='purple')
        ax4.set_title('Final Gate Weights Evolution')
        ax4.legend()

        # --- 4. Consistency Check ---
        ax5 = fig.add_subplot(gs[2, 1])
        # Correlation between scaling and gating
        ax5.scatter(df['scaling_unet_mean'], df['gate_wx'], alpha=0.3, label='UNet Scale vs Wx')
        ax5.scatter(df['scaling_vit_mean'], df['gate_wa'], alpha=0.3, label='ViT Scale vs Wa')
        ax5.set_title('Consistency: Scale vs Gate Weight')
        ax5.set_xlabel('Scaling Factor')
        ax5.set_ylabel('Gate Weight')
        ax5.legend()

        plt.suptitle(f"Scaling & Gating Analysis: {self.run_name}")
        plt.savefig(f"{self.output_dir}/03_scaling_gating.png")
        print(f"✓ Saved: 03_scaling_gating.png")

    # =========================================================
    # 4. GRADIENT FLOW & TRAINING HEALTH
    # =========================================================
    def plot_gradient_flow(self, gradient_log_path: str):
        """
        Monitors gradient health from the gradients.jsonl file.
        """
        print("Analyzing Gradient Flow...")
        df = self._parse_jsonl(gradient_log_path)
        if df is None: return

        # Keys from Logger: 'Unet_experts_grad_norm', 'VIT_experts_grad_norm', etc.
        unet_key = 'Unet_experts_grad_norm'
        vit_key = 'VIT_experts_grad_norm'

        if unet_key not in df.columns:
            print(f"⚠️ Gradient keys {unet_key} not found. Available keys: {df.columns.tolist()}")
            return

        fig, axes = plt.subplots(2, 2, figsize=(16, 10))

        # --- 1. Expert Gradients ---
        axes[0, 0].semilogy(df['step'], df[unet_key], label='UNet Experts', color='tab:orange', alpha=0.8)
        axes[0, 0].semilogy(df['step'], df[vit_key], label='ViT Experts', color='tab:blue', alpha=0.8)
        axes[0, 0].set_title('Expert Gradient Norms')
        axes[0, 0].legend()

        # --- 2. Router Gradients ---
        if 'Unet_router_grad_norm' in df.columns:
            axes[0, 1].semilogy(df['step'], df['Unet_router_grad_norm'], label='UNet Router', linestyle='--')
            axes[0, 1].semilogy(df['step'], df['vit_router_grad_norm'], label='ViT Router', linestyle='--')
            axes[0, 1].set_title('Router Gradient Norms')
            axes[0, 1].legend()

        # --- 3. Gradient Balance Ratio ---
        # Handle zero division safely
        ratio = df[unet_key] / (df[vit_key] + 1e-10)
        axes[1, 0].plot(df['step'], ratio, color='purple')
        axes[1, 0].axhline(1.0, color='green', linestyle='--')
        axes[1, 0].set_title('Gradient Ratio (UNet / ViT)')
        axes[1, 0].set_ylim(0, 5)  # Limit y-axis to see useful range

        # --- 4. Health Stats ---
        axes[1, 1].axis('off')

        vanishing_thresh = 1e-4
        exploding_thresh = 10.0

        # Calculate percentages
        van_u = (df[unet_key] < vanishing_thresh).mean() * 100
        van_v = (df[vit_key] < vanishing_thresh).mean() * 100

        stats = [
            ['Metric', 'UNet', 'ViT'],
            ['Vanishing (<1e-4)', f"{van_u:.1f}%", f"{van_v:.1f}%"],
            ['Mean Norm', f"{df[unet_key].mean():.4f}", f"{df[vit_key].mean():.4f}"],
            ['Max Norm', f"{df[unet_key].max():.4f}", f"{df[vit_key].max():.4f}"]
        ]

        table = axes[1, 1].table(cellText=stats, cellLoc='center', loc='center', bbox=[0, 0.2, 1, 0.6])
        table.auto_set_font_size(False)
        table.set_fontsize(11)

        for i in range(3):
            table[(0, i)].set_facecolor('#2E75B6')
            table[(0, i)].set_text_props(weight='bold', color='white')

        plt.suptitle(f"Gradient Health: {self.run_name}")
        plt.savefig(f"{self.output_dir}/04_gradients.png")
        print(f"✓ Saved: 04_gradients.png")

    # =========================================================
    # HELPER: Parse JSONL
    # =========================================================
    @staticmethod
    def _parse_jsonl(file_path: str) -> Optional[pd.DataFrame]:
        """Robustly parses a JSONL file into a Pandas DataFrame."""
        if not os.path.exists(file_path):
            print(f"❌ File not found: {file_path}")
            return None

        data = []
        try:
            with open(file_path, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            data.append(json.loads(line))
                        except json.JSONDecodeError:
                            continue

            if not data:
                print(f"⚠️ No data in {file_path}")
                return None

            return pd.DataFrame(data)

        except Exception as e:
            print(f"❌ Error reading {file_path}: {e}")
            return None


# =========================================================
# USAGE EXAMPLE
# =========================================================
if __name__ == "__main__":
    # Example usage
    plotter = Plotter(run_name="test_run")

    plotter.plot_comprehensive_training_dynamics(r"C:\Users\Maha Mamdouh\PycharmProjects\Heterogeneous-MOE-for-Diffusion-models\Utils\logs\hdmoem_flower102_v1_training.jsonl")
    plotter.plot_gradient_flow(r"C:\Users\Maha Mamdouh\PycharmProjects\Heterogeneous-MOE-for-Diffusion-models\Utils\logs\hdmoem_flower102_v1_gradients.jsonl")
    plotter.plot_scaling_and_gating_analysis(r"C:\Users\Maha Mamdouh\PycharmProjects\Heterogeneous-MOE-for-Diffusion-models\Utils\logs\hdmoem_flower102_v1_training.jsonl")